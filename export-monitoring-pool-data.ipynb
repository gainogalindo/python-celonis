{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Export Monitoring Pool Data",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## 1. Import",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import requests\nimport pandas as pd\nimport json\nimport pycelonis\nimport gc\nimport os\nfrom pycelonis import get_celonis\nfrom pycelonis.ems import ExportType\nfrom pycelonis.pql import PQL, PQLColumn, PQLFilter, OrderByColumn\nfrom pycelonis.utils.parquet import read_parquet\nfrom tqdm.auto import tqdm\nfrom concurrent.futures import ThreadPoolExecutor\nfrom IPython.display import clear_output",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## 2. Connection Variables",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Connect to Celonis\ncelonis = get_celonis(key_type='APP_KEY')",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## 3. Global Parameters",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Global parameters, such as CHUNK_SIZE (number of lines extracted on each file), PATH to store parquet files and maximum number of threads\n# Data Pool: Monitoring Pool\nMONITORING_POOL_ID = 'aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee' # <= Insert ID here\nPATH = '../dir/Monitoring_Pool_Data' # <= Insert PATH here\nIGNORE_FIELD = '_CELONIS_CHANGE_DATE'\nCHUNK_SIZE = 500000\nMAX_THREADS = 10",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## 4. Delete Previous Execution Files",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Clean before start\nfor file in os.listdir(PATH):\n    if file.endswith('.parquet'):\n        file_path = os.path.join(PATH, file)\n        os.remove(file_path)\n        #print('Deleted File:',file_path)\n    #endif\n#endfor",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## 5. Download Data From Monitoring Pool",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Get Monitoring Pool\ndata_pool = celonis.data_integration.get_data_pool(MONITORING_POOL_ID)\n\n# Build PQL Queries List containing all Table Columns\nqueries = []\n\n# Data Models From 'Monitoring Pool'\ndata_models = data_pool.get_data_models()\n\nfor data_model in tqdm(data_models):\n    # Tables from each Data Model\n    tables = data_model.get_tables()\n    \n    for table in tables:\n        # Get Columns to build PQL Query\n        cols = table.get_columns()\n        query = PQL(distinct=False)\n        \n        for col in cols:\n            # Format Query String: \"TABLE\".\"COLUMN\"\n            query_str = '\"{}\".\"{}\"'.format(table.name,col.name)\n            if col.name != IGNORE_FIELD:\n                query += PQLColumn(name=col.name, query=query_str)\n            #endif\n        #endfor\n        \n        # Append Query into Queries List\n        queries.append(\n            {\n                'data_model_id': data_model.id,\n                'table_name': table.name,\n                'query': query\n            }\n        )\n    #endfor\n#endfor",
      "metadata": {
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Loop Through Queries - Generate Files in Parallel\ngenerated_files = []\n\n# Downlaod Parquet Function\ndef download_parquet(obj):\n    # Get Data Model ID\n    data_model = data_pool.get_data_model(obj['data_model_id'])\n    \n    # Config Data Export Object\n    my_query = obj['query']\n    limit = CHUNK_SIZE\n    offset = 0\n    count = 1\n    control_file_size = 0\n    \n    while True:\n        # Init\n        my_query.limit = limit\n        my_query.offset = offset\n        data_export = data_model.create_data_export(query=my_query, export_type=ExportType.PARQUET)\n        \n        # Read Chunks\n        print('Extracting Table:', obj['table_name'])\n        data_export.wait_for_execution()\n        chunks = data_export.get_chunks()\n        \n        # Write Chunks to File\n        for chunk in chunks:\n        \n            # Control Variables\n            if control_file_size == 0:\n                # First Execution\n                control_file_size = chunk.getbuffer().nbytes\n            elif chunk.getbuffer().nbytes != control_file_size:\n                # Second to N-th execution\n                control_file_size = chunk.getbuffer().nbytes\n            elif chunk.getbuffer().nbytes == control_file_size:\n                # Repeated Execution on EMPTY file, exits function\n                clear_output(wait=True)\n                return\n            #endif\n            \n            # Write File\n            table_name = obj['table_name']\n            file = f'{table_name}_{count}.parquet'\n            file_path = os.path.join(PATH, file)\n            \n            generated_files.append({'table_name': table_name, 'index': count, 'file': file})\n            \n            with open(file_path, 'wb') as f:\n                f.write(chunk.read())\n            #endwith\n            \n            # Increment File Counter\n            count += 1\n            \n        #endfor\n        \n        # Call Garbage Collector\n        del chunks, chunk, data_export\n        gc.collect()\n        \n        # Increment offset\n        offset += CHUNK_SIZE\n    #endwhile\n#enddef",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Execute Threads\nwith ThreadPoolExecutor(MAX_THREADS) as executor:\n    for q in queries:\n        executor.submit(download_parquet, q)\n    #endfor\n#endwith",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## 6. Export Monitoring Pool Files to any Celonis Data Pool",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Create Generated Files Data Frame\ngenerated_files_df = pd.DataFrame(generated_files)\ngenerated_files_df = generated_files_df.sort_values(by=['table_name','index'],ascending=[True,True])\ngenerated_files_df.head()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Create Aggregation with Max Number of Files\nfiles_index_df = generated_files_df.groupby(['table_name']).max('index')\nfiles_index_df.head()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Set Data Pool to 'TARGET POOL'\ntarget_pool = 'ffffffff-gggg-hhhh-iiii-jjjjjjjjjjjj' # <= Insert ID here\ndata_pool = celonis.data_integration.get_data_pool(target_pool)\ndata_pool",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Push Local Data Into Destination Data Pool 'TARGET POOL'\nMAX_THREADS = 2\n\ndef push_table(table_name):\n    df = generated_files_df[generated_files_df['table_name'] == table_name]\n    for index, row in df.iterrows():\n        push_chunk(row['table_name'],row['index'],row['file'])\n    #endfor\n    \n    # Clear Output\n    clear_output(wait=True)\n#enddef\n\ndef push_chunk(table,index,file):\n    # Set File Path\n    file_path = os.path.join(PATH, file)\n    \n    # Get file info\n    size_mb = os.path.getsize(file_path) / 1024 ** 2 # megabytes\n    size_kb = os.path.getsize(file_path) / 1024 # kilobytes\n    \n    # Print file info\n    if size_mb < 0.1:\n        size = round(size_kb, 1)\n        print('File:', file.split('/').pop(), '(', size, 'kb )')\n    else:\n        size = round(size_mb, 1)\n        print('File:', file.split('/').pop(), '(', size, 'mb )')\n    #endif\n    \n    # Open file and convert Parquet into Data Frame\n    with open(file_path, 'rb') as f:\n        table_df = read_parquet(f)\n    #endwith\n    \n    # When Index = 1, overwrites table\n    if index == 1:\n        data_pool.create_table(\n            table_df,\n            table_name=table,\n            drop_if_exists=True,\n            force=True\n        )\n    \n    # When Index > 1, append table\n    else:\n        dp_table = data_pool.get_table(table)\n        dp_table.append(table_df)\n    #endif\n    \n    # Free memory\n    del table_df\n    gc.collect()\n#enddef",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "with ThreadPoolExecutor(MAX_THREADS) as executor:\n    for index, row in files_index_df.iterrows():\n        executor.submit(push_table, index)\n    #endfor\n#endwith",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}
